---
title: "Predictive Analytics Exam Module 1, Section 3"
---

Run CHUNK 1 to fit polynomials to random data.

```{r}
# CHUNK 1
library(glmnet)
library(ggplot2)
library(gridExtra)

set.seed(150) # You might want to remove this command later on to see if similar results obtain from other randomly generated data sets.

x <- runif(25, 0, 10) # Generate 25 predictor variables using the uniform distribution over the interval 0 to 10.
df <- data.frame(x = x, y = sin(x) + rnorm(25, 0, 0.5), fO = sin(x), X1 = x, X2 = x^2, X3 = x^3, X4 = x^4, X5 = x^5, X6 = x^6, X7 = x^7) # Generate 25 target variables and features needed to fit polynomial regression models. f0 holds the true value for the process.

m1 <- lm(as.formula("y~X1"), data = df)
m4 <- lm(as.formula("y~X1+X2+X3+X4"), data = df)
m7 <- lm(as.formula("y~X1+X2+X3+X4+X5+X6+X7"), data = df)

df$m0 <- mean(df$y) # Predictions using a 0-degree polynomial, which will be the mean of y.
df$m1 <- predict(m1, newdata = df)
df$m4 <- predict(m4, newdata = df)
df$m7 <- predict(m7, newdata = df)

p1 <- ggplot(data = df, aes(x = x)) +
  geom_point(aes(y = y), size = 3) +
  geom_smooth(aes(y = fO), se = FALSE, color = "blue", size = 1) +
  geom_smooth(aes(y = m0), se = FALSE, color = "red") +
  ggtitle("degree 0") +
  scale_y_continuous("y") +
  annotate("text", x = 2.5, y = 1.1, label = "True process", color = "blue") +
  annotate("text", x = 2, y = -0.15, label = "Fitted model", color = "red")
p2 <- ggplot(data = df, aes(x = x)) +
  geom_point(aes(y = y), size = 3) +
  geom_smooth(aes(y = fO), se = FALSE, color = "blue", size = 1) +
  geom_smooth(aes(y = m1), se = FALSE, color = "red") +
  ggtitle("degree 1") +
  scale_y_continuous("y") +
  annotate("text", x = 2.5, y = 1.1, label = "True process", color = "blue") +
  annotate("text", x = 2, y = -0.3, label = "Fitted model", color = "red")
p3 <- ggplot(data = df, aes(x = x)) +
  geom_point(aes(y = y), size = 3) +
  geom_smooth(aes(y = fO), se = FALSE, color = "blue", size = 1) +
  geom_smooth(aes(y = m4), se = FALSE, color = "red") +
  ggtitle("degree 4") +
  scale_y_continuous("y", limits = c(-2, 2)) +
  annotate("text", x = 2.5, y = 1.1, label = "True process", color = "blue") +
  annotate("text", x = 2, y = -0.9, label = "Fitted model", color = "red")
p4 <- ggplot(data = df, aes(x = x)) +
  geom_point(aes(y = y), size = 3) +
  geom_smooth(aes(y = fO), se = FALSE, color = "blue", size = 1) +
  geom_smooth(aes(y = m7), se = FALSE, color = "red") +
  ggtitle("degree 7") +
  scale_y_continuous("y", limits = c(-2, 2)) +
  annotate("text", x = 2.5, y = 1.1, label = "True process", color = "blue") +
  annotate("text", x = 2, y = -0.8, label = "Fitted model", color = "red")

grid.arrange(p1, p2, ncol = 2)
grid.arrange(p3, p4, ncol = 2)
```

Run CHUNK 2 to use simulation to estimate the expected loss and its three components for each of the three polynomial models. This may take a while.

```{r}
# CHUNK 2
# The outer loop uses test values from 0.05 to 9.95 by steps of 0.1.
set.seed(98765)
test.dat <- seq(0.05, 9.95, 0.1)
var <- matrix(0, nrow = 1, ncol = 4) # These hold the sums over the individual simulations.
bias <- matrix(0, nrow = 1, ncol = 4)
loss <- matrix(0, nrow = 1, ncol = 4)
error <- 0
for (i in 1:100)
{
  test.value <- test.dat[i] # Fix the test value to use.
  true.model.value <- sin(test.value) # Calculate the value using the true model.

  y.hat <- matrix(0, nrow = 500, ncol = 5) # Set up a matrix to hold the key values from the 500 simulations.
  for (j in 1:500) # Inner loop works with 500 simulated training sets.
  {
    x <- runif(25, 0, 10)
    df <- data.frame(x = x, y = sin(x) + rnorm(25, 0, 0.5), X1 = x, X2 = x^2, X3 = x^3, X4 = x^4, X5 = x^5, X6 = x^6, X7 = x^7)
    y.value <- true.model.value + rnorm(1, 0, 0.5)
    y.hat[j, 5] <- y.value # We need the observed target value for later use.
    m1 <- lm(as.formula("y~X1"), data = df)
    m4 <- lm(as.formula("y~X1+X2+X3+X4"), data = df)
    m7 <- lm(as.formula("y~X1+X2+X3+X4+X5+X6+X7"), data = df)
    test.vector <- data.frame(X1 = test.value, X2 = test.value^2, X3 = test.value^3, X4 = test.value^4, X5 = test.value^5, X6 = test.value^6, X7 = test.value^7)
    pred0 <- mean(df$y)
    pred1 <- predict(m1, newdata = test.vector)
    pred4 <- predict(m4, newdata = test.vector)
    pred7 <- predict(m7, newdata = test.vector)
    y.hat[j, 1] <- pred0
    y.hat[j, 2] <- pred1
    y.hat[j, 3] <- pred4
    y.hat[j, 4] <- pred7 # All the work to do now is to get the predicted values into the matrix.
  }
  var[1, 1] <- var[1, 1] + var(y.hat[, 1]) # Compute the variance and add to the running totals.
  var[1, 2] <- var[1, 2] + var(y.hat[, 2])
  var[1, 3] <- var[1, 3] + var(y.hat[, 3])
  var[1, 4] <- var[1, 4] + var(y.hat[, 4])
  bias[1, 1] <- bias[1, 1] + (mean(y.hat[, 1]) - true.model.value)^2 # Compute the biases and add to the running totals.
  bias[1, 2] <- bias[1, 2] + (mean(y.hat[, 2]) - true.model.value)^2
  bias[1, 3] <- bias[1, 3] + (mean(y.hat[, 3]) - true.model.value)^2
  bias[1, 4] <- bias[1, 4] + (mean(y.hat[, 4]) - true.model.value)^2
  error <- error + var(y.hat[, 5] - true.model.value) # Add the squared error to the running total.
  loss[1, 1] <- loss[1, 1] + sum((y.hat[, 1] - y.hat[, 5])^2) / 500 # Add the loss to the running total.
  loss[1, 2] <- loss[1, 2] + sum((y.hat[, 2] - y.hat[, 5])^2) / 500
  loss[1, 3] <- loss[1, 3] + sum((y.hat[, 3] - y.hat[, 5])^2) / 500
  loss[1, 4] <- loss[1, 4] + sum((y.hat[, 4] - y.hat[, 5])^2) / 500
}
var / 100 # Get the averages of the running totals and display them.
bias / 100
error / 100
loss / 100
```
